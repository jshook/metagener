I had a carefully thought-out explanation about why I wanted to build this system. Instead, read this.

Distributed systems testing is hard in practice. There are many tools in the wild which are designed to test simpler [single image] systems, but not many mature systems to enable testing distributed systems. A common approach is to try to use the existing tools that were build only for single-system testing in a bigger way, targeting them and modifying them to "work" with clusters of systems by making minor changes. This can be a fools errand, and usually is.

I'm all about keeping desings simple. There is a spectrum of competing concerns between simplicity and neceessary function. YAGNI and KISS work together to fight against intentional and unintentional complexity in designs.  However, on the flip side, some use these as large hammers of design-righteousness, vehemently opposing whatever function might go along with a measured increase in complexity. If we break down the competition a bit, it really ends up being a trade-off between the need of the function and the complexity of having it. Let's just be honest: There is no functionality that is free of complexity. Every single line of code you add to a project adds complexity in some measurable form. You can remove code and reduce net complexity. Some add more complexity than others, so yes, there is a differential. There is a reason that size and complexity are inextricably related.

Why bring this up? I'll try to explain. One of the reasons for unintentended complexity, in my mind, is lack of alignment between the conceptualization of a design and the actual problem or mode of use that it is meant to enable. In applied terms, this means that you can build a system for problem A, and then possibly adapt it to problem B, but that it will not be the system that you would have envisioned to solve problem B. I know many "agile practicioners" who would argue that you can effectively arrive at an idiomatic solution for B through "baby steps" or "stepwise refinement". Perhaps. I would like to see it demonstrated. I am a fan of agile thinking, but we all know how it gets distorted into Agile, big "A", sometimes in a self-defeating way.  We can debate the lack of design ownership that can emerge among the competing concerns of *ility, function, and revenue some other time. What I really want to emphasize here is that a well-conceived solution to one problem does not, by extension, automatically apply to another. If you can logically assert that one is a subset of the other, there is an argument to be made. However, it shouldn't be taken as an axiom that a solution conceived for problem A is automatically applicable to a superset of problem A. That is almost always wishful thinking. It's an example of trying to go the wrong direction through a type system.

Often, one of the most challenging aspects of solving a hard problem is finding the best approach. This includes choosing the right concepts and primitives that will be familiar to the tool users and the tool builders alike. It means that the primitives of the system are sufficient high-level enough to enable easy expression of common scenatios, yet low-level enough to express the unusual yet meaningful scenarios. 

Still, it is useful to pick some assumptions. There are usually some givens

However, you can identify some very obvious givens which are to be taken as axioms of your problem space and possibly even some about the form and function of the solution. Picking the right assumptions and knowing when they need to be reevaluated or discarded altogether is important. It's necessary to arrive at the right cross-cutting abstrations that put the machinery of your problem and solution on the same level of thinking. If the vocaulary of the tools you build are common to the vocabulary of your problem space, then you might actually be onto something.


On the contrary, a system design that emphasizes reuse of previous tools can be an inescapable limitation..

This brings us full circle to my rationale for starting this project-- what it is about, and what it aims to achieve. I endeavor to build a set of tools that make sense for testing distributed systems. It will not be based on previous implementations of testing tool, per se. This doesn't mean that it will avoid drawing from the knowledge of distributed systems. It only means that the problem is being evaluated as a whole, not through the lense of exising solutions. I hope that it is concieved and abstracted in as simple and effective terms as possible. If this is achieved, then the machinery of it will be uncompromised by extra complexity. I hope that what emerges is a

